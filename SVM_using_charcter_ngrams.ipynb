{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngram(txt, n, ngrams):\n",
    "    \n",
    "    for i in range(0,len(txt)-n+1):\n",
    "        ngram = txt[i:i+n]\n",
    "        ngrams.append(\" \".join(ngram))\n",
    "\n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizeArticle(filename):\n",
    "    \n",
    "    \n",
    "    # load text\n",
    "    file = open(filename, 'rt')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "\n",
    "    # Tokenize text string\n",
    "    #word_tokens = word_tokenize(text) \n",
    "    word_tokens = re.findall(r\"[A-Za-z '-]\",text)\n",
    "    \n",
    "    # Remove all punctunation and numbers\n",
    "    Words = [word.lower() for word in word_tokens]\n",
    "\n",
    "    words = []\n",
    "    for i in range(3,6):\n",
    "        w = ngram(Words,i,words)\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize variables to store author for each article and word tokenization matrix\n",
    "authorCount = 0\n",
    "authorID = []\n",
    "articleDB = []\n",
    "\n",
    "# Loop through each author directory within training set\n",
    "dataDir = os.listdir('data/C50train')\n",
    "for dDir in dataDir:\n",
    "\n",
    "    # Loop through each article within author directory\n",
    "    authorDir = os.listdir('data/C50train/' + dDir)\n",
    "    for aDir in authorDir:\n",
    "\n",
    "        # Tokenize each article and store author ID\n",
    "        articleDB.append(tokenizeArticle('data/C50train/' + dDir + '/' + aDir))\n",
    "        authorID.append(authorCount)\n",
    "        \n",
    "    # Increment author id for next author\n",
    "    authorCount = authorCount + 1\n",
    "\n",
    "# Loop through each author directory within testing set\n",
    "authorCount = 0\n",
    "dataDir = os.listdir('data/C50test')\n",
    "for dDir in dataDir:\n",
    "\n",
    "    # Loop through each article within author directory\n",
    "    authorDir = os.listdir('data/C50test/' + dDir)\n",
    "    for aDir in authorDir:\n",
    "\n",
    "        # Tokenize each article and store author ID\n",
    "        articleDB.append(tokenizeArticle('data/C50test/' + dDir + '/' + aDir))\n",
    "        authorID.append(authorCount)\n",
    "        \n",
    "    # Increment author id for next author\n",
    "    authorCount = authorCount + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(articleDB[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterLen(DF, docs, val1, val2):\n",
    "    r\"\"\" filter out terms that are too short. \n",
    "    docs is a list of lists, each inner list is a document represented as a list of words\n",
    "    minlen is the minimum length of the word to keep\n",
    "    \"\"\"\n",
    "    return [ [t for t in d if val2 <= DF[t] <= val1] for d in docs ]\n",
    "\n",
    "df = defaultdict(int)\n",
    "for d in articleDB:\n",
    "    for w in set(d):\n",
    "        df[w] += 1\n",
    "\n",
    "DB = filterLen(df, articleDB, 5000, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from scipy.sparse import csr_matrix\n",
    "def build_matrix(docs):\n",
    "    r\"\"\" Build sparse matrix from a list of documents, \n",
    "    each of which is a list of word/terms in the document.  \n",
    "    \"\"\"\n",
    "    nrows = len(docs)\n",
    "    idx = {}\n",
    "    tid = 0\n",
    "    nnz = 0\n",
    "    for d in docs:\n",
    "        nnz += len(set(d))\n",
    "        for w in d:\n",
    "            if w not in idx:\n",
    "                idx[w] = tid\n",
    "                tid += 1\n",
    "    ncols = len(idx)\n",
    "        \n",
    "    # set up memory\n",
    "    ind = np.zeros(nnz, dtype=np.int)\n",
    "    val = np.zeros(nnz, dtype=np.double)\n",
    "    ptr = np.zeros(nrows+1, dtype=np.int)\n",
    "    i = 0  # document ID / row counter\n",
    "    n = 0  # non-zero counter\n",
    "    # transfer values\n",
    "    for d in docs:\n",
    "        cnt = Counter(d)\n",
    "        keys = list(k for k,_ in cnt.most_common())\n",
    "        l = len(keys)\n",
    "        for j,k in enumerate(keys):\n",
    "            ind[j+n] = idx[k]\n",
    "            val[j+n] = cnt[k]\n",
    "        ptr[i+1] = ptr[i] + l\n",
    "        n += l\n",
    "        i += 1\n",
    "            \n",
    "    mat = csr_matrix((val, ind, ptr), shape=(nrows, ncols), dtype=np.double)\n",
    "    mat.sort_indices()\n",
    "    \n",
    "    return mat\n",
    "\n",
    "\n",
    "def csr_info(mat, name=\"\", non_empy=False):\n",
    "    r\"\"\" Print out info about this CSR matrix. If non_empy, \n",
    "    report number of non-empty rows and cols as well\n",
    "    \"\"\"\n",
    "    if non_empy:\n",
    "        print(\"%s [nrows %d (%d non-empty), ncols %d (%d non-empty), nnz %d]\" % (\n",
    "                name, mat.shape[0], \n",
    "                sum(1 if mat.indptr[i+1] > mat.indptr[i] else 0 \n",
    "                for i in range(mat.shape[0])), \n",
    "                mat.shape[1], len(np.unique(mat.indices)), \n",
    "                len(mat.data)))\n",
    "    else:\n",
    "        print( \"%s [nrows %d, ncols %d, nnz %d]\" % (name, \n",
    "                mat.shape[0], mat.shape[1], len(mat.data)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [nrows 5000, ncols 3000, nnz 8243491]\n"
     ]
    }
   ],
   "source": [
    "mat = build_matrix(DB)\n",
    "csr_info(mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows = mat.shape[0]\n",
    "nnz = mat.nnz\n",
    "ind, val, ptr = mat.indices, mat.data, mat.indptr\n",
    "# document frequency\n",
    "df = defaultdict(int)\n",
    "for i in ind:\n",
    "    df[i] += 1\n",
    "\n",
    "# inverse document frequency\n",
    "for k,v in df.items():\n",
    "    df[k] = np.log(nrows / float(v))  ## df turns to idf - reusing memory\n",
    "    \n",
    "# scale by idf\n",
    "for i in range(0, nnz):\n",
    "    val[i] *= df[ind[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows = mat.shape[0]\n",
    "nnz = mat.nnz\n",
    "ind, val, ptr = mat.indices, mat.data, mat.indptr\n",
    "for i in range(nrows):\n",
    "        rsum = 0.0    \n",
    "        for j in range(ptr[i], ptr[i+1]):\n",
    "            rsum += val[j]**2\n",
    "        if rsum == 0.0:\n",
    "            continue  # do not normalize empty rows\n",
    "        rsum = 1.0/np.sqrt(rsum)\n",
    "        for j in range(ptr[i], ptr[i+1]):\n",
    "            val[j] *= rsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the train data into X_train and y_train datasets in 80:20 ratio.\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    mat, authorID, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error rate without dimensionality reduction: 176/1000 * 100 = 17.600000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "svclassifier = SVC(kernel='rbf',C=10)\n",
    "svclassifier.fit(X_train, y_train)\n",
    "pr = svclassifier.predict(X_test)\n",
    "\n",
    "errors = (y_test != pr).sum()\n",
    "total = X_test.shape[0]\n",
    "error_rate_without_dr = (errors/float(total)) * 100\n",
    "print(\"Error rate without dimensionality reduction: %d/%d * 100 = %f\" % (errors, total, error_rate_without_dr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision score =  0.8312432357854997\n",
      "recall score =  0.827\n",
      "f1 score =  0.8248527274873307\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "print('precision score = ', precision_score(y_test, pr, average = 'weighted'))\n",
    "print('recall score = ', recall_score(y_test, pr, average = 'weighted'))\n",
    "print('f1 score = ', f1_score(y_test, pr, average = 'weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "svc = SVC(kernel='rbf',C=10)\n",
    "k_fold = KFold(n_splits=10, shuffle=True, random_state=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFold(n_splits=10, random_state=20, shuffle=True)\n"
     ]
    }
   ],
   "source": [
    "print(k_fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.87495964, 0.84416594, 0.81724579, 0.82454249, 0.85847478,\n",
       "       0.83693481, 0.86419569, 0.84608703, 0.86882243, 0.85784559])"
      ]
     },
     "execution_count": 503,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(svc, mat, authorID, cv=k_fold, n_jobs=-1, scoring='f1_weighted')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
