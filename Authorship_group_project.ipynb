{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   !pip install cleantext #only need to do this once (on startup?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse as sp\n",
    "from numpy.linalg import norm\n",
    "from collections import Counter, defaultdict\n",
    "from scipy.sparse import csr_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import cleantext\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate svd with logistic regression algorithm for classification\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overview:\n",
    "For my portion of the group project, I elected to see if I could make kmeans clustering work to:\n",
    "1.) Identify the 50 authors represented in the per author 50(sample), 50(test) set of exemplar articles by predicting which author cluster of a given test set article would likely belong to by using the kmean \"predict\" function.\n",
    "\n",
    "I identified the following steps:\n",
    "\n",
    "1.) Ingest the data\n",
    "And after data ingestion populate an appropriate data structure to work with it.\n",
    "The data set is located at: https://archive.ics.uci.edu/ml/datasets/Reuter_50_50\n",
    "The training set and the data set are structured similarly:\n",
    "From a shared set of 50 authors, 50 sample and 50 training article exemplars are provided from each author.\n",
    "\n",
    "2.) Prepare the data\n",
    "a.) Clean it up. I experimented with two pre-made Python clean-up functions found at:\n",
    "https://pypi.org/project/clean-text/\n",
    "as well as the cleantext function/package found at:\n",
    "https://pypi.org/project/cleantext/\n",
    "\n",
    "The clean-text function has as defaults the following flags:\n",
    "\n",
    "from cleantext import clean\n",
    "clean(\"some input\",\n",
    "    fix_unicode=True,               # fix various unicode errors\n",
    "    to_ascii=True,                  # transliterate to closest ASCII representation\n",
    "    lower=True,                     # lowercase text\n",
    "    no_line_breaks=False,           # fully strip line breaks as opposed to only normalizing them\n",
    "    no_urls=False,                  # replace all URLs with a special token\n",
    "    no_emails=False,                # replace all email addresses with a special token\n",
    "    no_phone_numbers=False,         # replace all phone numbers with a special token\n",
    "    no_numbers=False,               # replace all numbers with a special token\n",
    "    no_digits=False,                # replace all digits with a special token\n",
    "    no_currency_symbols=False,      # replace all currency symbols with a special token\n",
    "    no_punct=False,                 # remove punctuations\n",
    "    replace_with_punct=\"\",          # instead of removing punctuations you may replace them\n",
    "    replace_with_url=\"<URL>\",\n",
    "    replace_with_email=\"<EMAIL>\",\n",
    "    replace_with_phone_number=\"<PHONE>\",\n",
    "    replace_with_number=\"<NUMBER>\",\n",
    "    replace_with_digit=\"0\",\n",
    "    replace_with_currency_symbol=\"<CUR>\",\n",
    "    lang=\"en\"                       # set to 'de' for German special handling\n",
    ")\n",
    "    \n",
    "In the case of clean-text, I used the default settings.\n",
    "    \n",
    "Whereas the cleantext function/package has as defaults the following flags:\n",
    "\n",
    "    To return a list of words from the text,\n",
    "cleantext.clean_words(\"your_raw_text_here\", all= True) \n",
    "\n",
    "To choose a specific set of cleaning operations,\n",
    "\n",
    "cleantext.clean_words(\"your_raw_text_here\",\n",
    "all= False # Execute all cleaning operations\n",
    "extra_spaces=True ,  # Remove extra white space \n",
    "stemming=True , # Stem the words\n",
    "stopwords=True ,# Remove stop words\n",
    "lowercase=True ,# Convert to lowercase\n",
    "numbers=True ,# Remove all digits \n",
    "punct=True ,# Remove all punctuations\n",
    "stp_lang='english'  # Language for stop words\n",
    ")\n",
    "\n",
    "In the case of cleantext I used the \"all-True\" flags.\n",
    "\n",
    "b.)Prepare the test and sample data for injestion by kmeans.\n",
    "The training data was prepared in two ways:\n",
    "    1.) Each article was ingested as a list (so, a list of 2500 article-vectors)\n",
    "    2.) All of the training articles for a given author were ingested into a single list (so, a list of 50 vectors.)\n",
    "    \n",
    "    \n",
    "    I am preparing four cases:\n",
    "    1.) a cmer-ized version of the 50 and 2500 vector cmer = (2,3,4) \n",
    "    2.) a non-cmer-ized version\n",
    "    (The cmer function is from class.  See definition below.)\n",
    "    \n",
    "c.) Vectorize the data\n",
    "I am using "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If our first author-independent goal is to build a giant vector of possible words from training set, then maybe the correct approach is to sequentially read through all of the different word files and make a set or dictionary of all terms we encounter.  If we build a dictionary, then the word itself can be the key, and each time the word is encuontered the value is updated by 1.\n",
    "\n",
    "The advantage of doing a dictionary like that would be that we could do a dimensionality reduction on those elements that have entries across many different authors.  We might not want to get rid of text entries that are common for one single author.  So, perhaps the value of each dictionary key would be a 50-tuple with the number of times each word appeared for each author."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper Functions 1\n",
    "#Functions Related to cmerization.  (The original from a provided activity, and a wrapper for it.)\n",
    "def cmer(name, c=3):\n",
    "    r\"\"\" Given a name and parameter c, return the vector of c-mers associated with the name\n",
    "    \"\"\"\n",
    "    #print(len(name),c)\n",
    "    if c > len(name):\n",
    "        #print(name)\n",
    "        #print(\"c must be less than equal to the character length of \", name)\n",
    "        return -1\n",
    "    if c <= 0:\n",
    "        #print(\"The integer value of c must be greater than 0\")\n",
    "        return -2\n",
    "    #I think this is already done in other function name = name.lower()\n",
    "    v = []\n",
    "    for i in range(len(name)-c+1):\n",
    "        v.append(name[i:i+c])    \n",
    "    return v\n",
    "#\n",
    "#\n",
    "#take a vector of strings and return a vector of cmer-ized length \"c\" strings\n",
    "#I wrapped the cmer function, above in a higher level function to always output a single flat vector without errors.\n",
    "x = [\"tes\", \"bossy\", \"goodness\"]\n",
    "def cmerize_vector(vector_in,c):\n",
    "    cmerized_vector = []\n",
    "    for el in vector_in:\n",
    "        #print(el)\n",
    "        if len(el) <= c:\n",
    "            cmerized_vector.append(el)\n",
    "        else:\n",
    "            temp_cmer = cmer(el,c)\n",
    "            if temp_cmer != -1 and temp_cmer != -2:\n",
    "                for frag in temp_cmer:\n",
    "                    cmerized_vector.append(frag)\n",
    "    return cmerized_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = ['hello', 'goodnight']\n",
    "cmerize_vector(x,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper Function 3\n",
    "#These functions are from the class activity nn-classication\n",
    "def build_matrix(docs):\n",
    "    r\"\"\" Build sparse matrix from a list of documents, \n",
    "    each of which is a list of word/terms in the document.  \n",
    "    \"\"\"\n",
    "    nrows = len(docs) #a \"doc\" here is a list of words.  \n",
    "    idx = {}\n",
    "    tid = 0\n",
    "    nnz = 0\n",
    "    for d in docs:\n",
    "        print(d) #for test\n",
    "        nnz += len(set(d))\n",
    "        for w in d:\n",
    "            if w not in idx:\n",
    "                idx[w] = tid\n",
    "                tid += 1\n",
    "    ncols = len(idx)\n",
    "        \n",
    "    # set up memory\n",
    "    ind = np.zeros(nnz, dtype=np.int)\n",
    "    val = np.zeros(nnz, dtype=np.double)\n",
    "    ptr = np.zeros(nrows+1, dtype=np.int)\n",
    "    i = 0  # document ID / row counter\n",
    "    n = 0  # non-zero counter\n",
    "    # transfer values\n",
    "    for d in docs:\n",
    "        cnt = Counter(d)\n",
    "        keys = list(k for k,_ in cnt.most_common())\n",
    "        l = len(keys)\n",
    "        for j,k in enumerate(keys):\n",
    "            ind[j+n] = idx[k]\n",
    "            val[j+n] = cnt[k]\n",
    "        ptr[i+1] = ptr[i] + l\n",
    "        n += l\n",
    "        i += 1\n",
    "            \n",
    "    mat = csr_matrix((val, ind, ptr), shape=(nrows, ncols), dtype=np.double)\n",
    "    mat.sort_indices()\n",
    "    \n",
    "    return mat\n",
    "\n",
    "def csr_info(mat, name=\"\", non_empty=False):\n",
    "    r\"\"\" Print out info about this CSR matrix. If non_empty, \n",
    "    report the number of non-empty rows and cols as well\n",
    "    \"\"\"\n",
    "    if non_empty:\n",
    "        print(\"%s [nrows %d (%d non-empty), ncols %d (%d non-empty), nnz %d]\" % (\n",
    "                name, mat.shape[0], \n",
    "                sum(1 if mat.indptr[i+1] > mat.indptr[i] else 0 \n",
    "                for i in range(mat.shape[0])), \n",
    "                mat.shape[1], len(np.unique(mat.indices)), \n",
    "                len(mat.data)))\n",
    "    else:\n",
    "        print( \"%s [nrows %d, ncols %d, nnz %d]\" % (name, \n",
    "                mat.shape[0], mat.shape[1], len(mat.data)) )\n",
    "\n",
    "def csr_l2normalize(mat, copy=False, **kargs):\n",
    "    r\"\"\" Normalize the rows of a CSR matrix by their L-2 norm. \n",
    "    If copy is True, returns a copy of the normalized matrix.\n",
    "    \"\"\"\n",
    "    if copy is True:\n",
    "        mat = mat.copy()\n",
    "    nrows = mat.shape[0]\n",
    "    nnz = mat.nnz\n",
    "    ind, val, ptr = mat.indices, mat.data, mat.indptr\n",
    "    # normalize\n",
    "    for i in range(nrows):\n",
    "        rsum = 0.0    \n",
    "        for j in range(ptr[i], ptr[i+1]):\n",
    "            rsum += val[j]**2\n",
    "        if rsum == 0.0:\n",
    "            continue  # do not normalize empty rows\n",
    "        rsum = 1.0/np.sqrt(rsum)\n",
    "        for j in range(ptr[i], ptr[i+1]):\n",
    "            val[j] *= rsum\n",
    "            \n",
    "    if copy is True:\n",
    "        return mat\n",
    "      \n",
    "#I am not sure if I like this function, because it returns vectors in place of individual words added to a vector\n",
    "#Is it better this way?\n",
    "def namesToMatrix(names, c):\n",
    "    docs = [cmer(n, c) for n in names]\n",
    "    print(docs)\n",
    "    return build_matrix(docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preliminary data ingest (training file names, test file names, author names) operations\n",
    "import glob\n",
    "#\n",
    "#Three container lists for training file paths, test file paths, and author names in both cases:\n",
    "txtfiles_train = []\n",
    "txtfiles_test =[]\n",
    "author_names=[]\n",
    "#for file in glob.glob(\"authorship_data/C50/C50train/AaronPressman/*.txt\"):\n",
    "for file in glob.glob(\"authorship_data/C50/C50train/*/*.txt\"):\n",
    "    txtfiles_train.append(file)\n",
    "#for file in glob.glob(\"authorship_data/C50/C50train/AaronPressman/*.txt\"):\n",
    "for file in glob.glob(\"authorship_data/C50/C50test/*/*.txt\"):\n",
    "    txtfiles_test.append(file)\n",
    "#print(txtfiles_train)\n",
    "for dirName in glob.glob(\"authorship_data/C50/C50train/*/\"):\n",
    "    author_names.append(dirName[29:-1])\n",
    "#author_words = dict(author_names)\n",
    "#print(author_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.shape(txtfiles_test))\n",
    "#print(txtfiles_test[110:113])\n",
    "np.shape(txtfiles_train)#this is a test of the sparse matrix generation.  I would like to run it on cleaned up lists of words in each document "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build Containers for 3 types of matrices we will us\n",
    "# Create an empty list, for the words from all articles from each author\n",
    "list_of_authors_words_Reference = [] #for 50 vectors of words from author training files\n",
    "list_of_authors_words_Test      = [] #for 2500 vectors for each test article for all authors \n",
    "list_of_authors_words_Sample_0   =[] #for 2500 vectors for each training article for all authors\n",
    "#populate lists with empty 'placeholder' lists for later ingestion\n",
    "for i in range(50):\n",
    "    # In each iteration, add an empty list to the main list\n",
    "    list_of_authors_words_Reference.append([''])\n",
    "for j in range(2500):#(50-num_in_sample)*50):\n",
    "    # In each iteration, add an empty list to the main list\n",
    "    list_of_authors_words_Test.append([''])\n",
    "    list_of_authors_words_Sample_0.append([''])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Verify shape of container directories is correct\n",
    "print(np.shape(list_of_authors_words_Reference))\n",
    "print(np.shape(list_of_authors_words_Test))\n",
    "print(np.shape(list_of_authors_words_Sample_0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper Function\n",
    "def filterLen(docs, minlen):\n",
    "    r\"\"\" filter out terms that are too short. \n",
    "    docs is a list of lists, each inner list is a document represented as a list of words\n",
    "    minlen is the minimum length of the word to keep\n",
    "    \"\"\"\n",
    "    return [ [t for t in d if len(t) >= minlen ] for d in docs ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Populate Containers\n",
    "#CASE 1 of 3.  Ingest and preprocess files\n",
    "#2500 Sample vectors where each vector is an article and there are fifty article per author and 50 authors.\n",
    "#\n",
    "#This is the set of all \"words\" in each files after removal of stop words and cmerizing.\n",
    "#SAMPLE AGGREGATED CASE\n",
    "allWords=set([''])\n",
    "temp_authors_words_Sample = set([''])\n",
    "last_sample_index = 0\n",
    "# read in the dataset\n",
    "cmer_size = 4\n",
    "i=0\n",
    "j=-1\n",
    "tf_last = \"No file\"\n",
    "last_author_processed=\"None\"\n",
    "Sample = False\n",
    "Test = False\n",
    "for tf in txtfiles_train:\n",
    "    j = j+1\n",
    "    tf_current = tf\n",
    "    current_author_base = tf[29:-1]\n",
    "    current_author_name_slash_index=current_author_base.index(\"\\\\\")\n",
    "    current_author = current_author_base[0:current_author_name_slash_index]\n",
    "    if i == 0:\n",
    "        last_author_processed = current_author\n",
    "    #with open(\"authorship_data/C50/C50train/AaronPressman/2537newsMl.txt\", \"r\", encoding=\"utf8\") as fh: \n",
    "    if current_author != last_author_processed:\n",
    "        temp_authors_words_Sample_post_cmer = cmerize_vector(list(temp_authors_words_Sample),cmer_size)\n",
    "        list_of_authors_words_Reference[last_sample_index] = temp_authors_words_Sample_post_cmer \n",
    "        #list_of_authors_words_Reference[last_sample_index] = sorted(list(temp_authors_words_Sample)) #added sorted, don't know if it helps\n",
    "        last_sample_index = last_sample_index + 1\n",
    "        temp_authors_words_Sample = set([''])\n",
    "        last_author_processed = current_author\n",
    "    with open(tf, \"r\", encoding=\"utf8\") as fh: \n",
    "        lines = fh.readlines()\n",
    "        for l in lines:\n",
    "            #if return_index(randnums_sample_indices,int(i/50)) != -1:\n",
    "            #print(\"last_sample_index = \",last_sample_index,\"tf sample =\", tf)               \n",
    "            #This is The processing step:\n",
    "            #cleaning and aggregating a single big vector per author\n",
    "            #l1 = filterLen(l,3)\n",
    "            #print(\"l1 = \", filterLen(l,10))\n",
    "            tempAuthorWordSet_Sample = set(cleantext.clean_words(l, all=True))  #test\n",
    "            #This is the aggregation of lines step:\n",
    "            temp_authors_words_Sample=temp_authors_words_Sample.union(tempAuthorWordSet_Sample)\n",
    "        #print('\\nAfter all unions: last_sample_index  = ', last_sample_index)\n",
    "    i = i+1\n",
    "temp_authors_words_Sample_post_cmer = cmerize_vector(list(tempAuthorWordSet_Sample),cmer_size)\n",
    "list_of_authors_words_Reference[last_sample_index] = temp_authors_words_Sample_post_cmer \n",
    "\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Populate Containers\n",
    "#Case 2 of 3 Ingest and preprocess files\n",
    "#This is the set of all words in all files after removal of stop words, etc.\n",
    "#TEST CASE\n",
    "allWords=set([''])\n",
    "temp_authors_words_Test = set([''])\n",
    "last_test_index = 0\n",
    "# read in the dataset\n",
    "cmer_size=4\n",
    "i=0\n",
    "#j=-1\n",
    "last_tf_processed = \"No file Name, yet\"\n",
    "tf_current = \"No file Name, yet\"\n",
    "Sample = False\n",
    "Test = False\n",
    "for tf in txtfiles_test:\n",
    "    #if i%50 == 0:\n",
    "        #print(\"processing file: \", tf)\n",
    "    j = j+1\n",
    "    tf_current = tf\n",
    "    #current_author_base = tf[29:-1]\n",
    "    #current_author_name_slash_index=current_author_base.index(\"\\\\\")\n",
    "    #current_author = current_author_base[0:current_author_name_slash_index]\n",
    "    if i == 0:\n",
    "        last_tf_processed = tf_current\n",
    "    #with open(\"authorship_data/C50/C50train/AaronPressman/2537newsMl.txt\", \"r\", encoding=\"utf8\") as fh: \n",
    "    #if current_author != last_author_processed:\n",
    "    if tf_current != last_tf_processed:\n",
    "        #print(last_test_index)\n",
    "        temp_authors_words_Test_post_cmer = cmerize_vector(list(temp_authors_words_Test), cmer_size)\n",
    "        list_of_authors_words_Test[last_test_index] = temp_authors_words_Test_post_cmer #added sorted, don't know if it helps\n",
    "        #list_of_authors_words_Test[last_test_index] = sorted(list(temp_authors_words_Test)) #added sorted, don't know if it helps\n",
    "        temp_authors_words_Test = set([''])\n",
    "        last_tf_processed = tf_current\n",
    "        #print('\\nAfter all unions: last_test_index  = ', last_test_index)\n",
    "        last_test_index = last_test_index + 1\n",
    "    with open(tf, \"r\", encoding=\"utf8\") as fh: \n",
    "        lines = fh.readlines()\n",
    "        for l in lines:\n",
    "            #if return_index(randnums_sample_indices,int(i/50)) != -1:\n",
    "            #print(\"last_test_index = \",last_test_index,\"tf test file =\", tf)               \n",
    "            #This is cleaning and aggregating a single big vector per author\n",
    "            #l1 = filterLen(l,3)\n",
    "            tempAuthorWordSet_Test = set(cleantext.clean_words(l, all=True))  #test\n",
    "            temp_authors_words_Test=temp_authors_words_Test.union(tempAuthorWordSet_Test)\n",
    "            #print('\\nAfter all unions: last_test_index  = ', last_test_index)\n",
    "    i = i+1\n",
    "temp_authors_words_Test_post_cmer = cmerize_vector(list(temp_authors_words_Test), cmer_size)\n",
    "list_of_authors_words_Test[last_test_index] = temp_authors_words_Test_post_cmer #added sorted, don't know if it helps "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Populate Containers\n",
    "#3 of 3 Ingest and pre-process files\n",
    "#This is the set of all words in all files after removal of stop words, etc.\n",
    "#Standard Sample CASE\n",
    "#INPROCESS\n",
    "allWords=set([''])\n",
    "temp_authors_words_Sample_0 = set([''])\n",
    "last_sample_index = 0\n",
    "# read in the dataset\n",
    "cmer_size=4\n",
    "i=0\n",
    "#j=-1\n",
    "last_sf_processed = \"No file Name, yet\"\n",
    "sf_current = \"No file Name, yet\"\n",
    "Sample = False\n",
    "Test = False\n",
    "for tf in txtfiles_train:\n",
    "    j = j+1\n",
    "    tf_current = tf\n",
    "    #current_author_base = tf[29:-1]\n",
    "    #current_author_name_slash_index=current_author_base.index(\"\\\\\")\n",
    "    #current_author = current_author_base[0:current_author_name_slash_index]\n",
    "    if i == 0:\n",
    "        last_tf_processed = tf_current\n",
    "    #with open(\"authorship_data/C50/C50train/AaronPressman/2537newsMl.txt\", \"r\", encoding=\"utf8\") as fh: \n",
    "    #if current_author != last_author_processed:\n",
    "    if tf_current != last_tf_processed:\n",
    "        temp_list_of_authors_words_Sample_0_post_cmer = cmerize_vector(list(temp_authors_words_Sample_0),cmer_size)\n",
    "        list_of_authors_words_Sample_0[last_sample_index] = temp_list_of_authors_words_Sample_0_post_cmer\n",
    "        #list_of_authors_words_Sample_0[last_sample_index] = sorted(list(temp_authors_words_Sample_0)) #added sorted, don't know if it helps\n",
    "        temp_authors_words_Sample_0 = set([''])\n",
    "        last_tf_processed = tf_current\n",
    "        #print('\\nAfter all unions: last_sample_index  = ', last_sample_index)\n",
    "        last_sample_index = last_sample_index + 1\n",
    "    with open(tf, \"r\", encoding=\"utf8\") as fh: \n",
    "        lines = fh.readlines()\n",
    "        for l in lines:\n",
    "            #if return_index(randnums_sample_indices,int(i/50)) != -1:\n",
    "            #print(\"last_sample_index = \",last_sample_index,\"sf sample file =\", tf)               \n",
    "            #This is cleaning and aggregating a single big vector per author\n",
    "            #l1 = filterLen(l,3)\n",
    "            tempAuthorWordSet_Sample_0 = set(cleantext.clean_words(l, all=True))  #test\n",
    "            temp_authors_words_Sample_0=temp_authors_words_Sample_0.union(tempAuthorWordSet_Sample_0)\n",
    "            #print('\\nAfter all unions: last_test_index  = ', last_test_index)\n",
    "    i = i+1\n",
    "#last one not in loop\n",
    "#list_of_authors_words_Sample_0[last_sample_index] = sorted(list(temp_authors_words_Sample_0)) #added sorted, don't know if it helps\n",
    "temp_list_of_authors_words_Sample_0_post_cmer = cmerize_vector(list(temp_authors_words_Sample_0),cmer_size)\n",
    "list_of_authors_words_Sample_0[last_sample_index] = temp_list_of_authors_words_Sample_0_post_cmer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting functions\n",
    "def plotWf(docs, plot=True, logscale=True):\n",
    "    r\"\"\"Get collection-wide word frequencies and optionally plot them.\"\"\"\n",
    "    words = defaultdict(int)\n",
    "    for d in docs:\n",
    "        for w in d:\n",
    "            words[w] += 1\n",
    "    if plot is True:\n",
    "        plt.plot(sorted(words.values(), reverse=True))\n",
    "        plt.xlabel('word')\n",
    "        plt.ylabel('frequency')\n",
    "        if logscale is True:\n",
    "            plt.yscale('log')\n",
    "            plt.ylabel('log(frequency)')\n",
    "        plt.title('Corpus-wide word frequency distribution')\n",
    "        plt.show()\n",
    "    return words\n",
    "\n",
    "def plotDf(docs, plot=True, logscale=False):\n",
    "    r\"\"\"Get collection-wide document-word frequencies and optionally plot them.\"\"\"\n",
    "    # document word frequency\n",
    "    df = defaultdict(int)\n",
    "    for d in docs:\n",
    "        for w in set(d):\n",
    "            df[w] += 1\n",
    "    if plot is True:\n",
    "        plt.plot(sorted(df.values(), reverse=True))\n",
    "        plt.xlabel('word')\n",
    "        plt.ylabel('frequency')\n",
    "        if logscale is True:\n",
    "            plt.yscale('log')\n",
    "            plt.ylabel('log(frequency)')\n",
    "        plt.title('Corpus-wide document-word frequency distribution')\n",
    "        plt.show()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(type(list2))\n",
    "_ = plotWf(list_of_authors_words_Sample_0)\n",
    "_ = plotDf(list_of_authors_words_Sample_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plotWf(list_of_authors_words_Reference)\n",
    "_ = plotDf(list_of_authors_words_Reference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plotWf(list_of_authors_words_Test)\n",
    "_ = plotDf(list_of_authors_words_Test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is where we now begin to prepare the data for scikitlearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_out_test             = build_matrix(list_of_authors_words_Test)\n",
    "mat_out_sample           = build_matrix(list_of_authors_words_Sample_0)\n",
    "mat_out_sample_by_author = build_matrix(list_of_authors_words_Reference)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make normalized vectors\n",
    "#print(mat.shape)\n",
    "mat_out_test_norm = csr_l2normalize(mat_out_test, copy=True)\n",
    "print(mat_out_test_norm.shape)\n",
    "csr_info(mat_out_test_norm, name=\"test\", non_empty=True)\n",
    "#\n",
    "mat_out_sample_norm = csr_l2normalize(mat_out_sample, copy=True)\n",
    "print(mat_out_sample_norm.shape)\n",
    "csr_info(mat_out_sample_norm, name=\"sample\", non_empty=True)\n",
    "#\n",
    "mat_out_sample_by_author_norm = csr_l2normalize(mat_out_sample_by_author, copy=True)\n",
    "print(mat_out_sample_by_author_norm.shape)\n",
    "csr_info(mat_out_sample_by_author_norm, name=\"sample_by_author\", non_empty=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we will look at clustering after svd dimension (feature) reduction of all three data sets in two cases.  We perform the same data reduction to both data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize instance of SVDTruncated\n",
    "svd = TruncatedSVD(n_components=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Run SVDTruncated for dimensionality reduction on\n",
    "#mat_out_sample_norm\n",
    "#and\n",
    "#mat_out_test_norm\n",
    "\n",
    "\n",
    "#initialize instance of k-means:\n",
    "km=KMeans(n_clusters=50)\n",
    "\n",
    "#Apply to sample\n",
    "svd.fit_transform(mat_out_sample_norm)\n",
    "km.fit(mat_out_sample_norm) \n",
    "y_km_test_out=km.fit_predict(mat_out_test_norm) \n",
    "\n",
    "print(np.shape(y_km_test_out))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1=collections.Counter(y_km_test_out)\n",
    "#print(t1.values())\n",
    "print((collections.Counter(y_km_test_out)),'\\n')\n",
    "plt.bar(t1.keys(),t1.values())\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.std(list(t1.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.shape(mat_sample_reduced_out))\n",
    "print(np.shape(mat_test_reduced_out))\n",
    "print(np.shape(y_km_sample_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now that dimensionality reduction is complete:\n",
    "#make a kmeans instance\n",
    "kmeans_1=KMeans(n_clusters=50,init='k-means++',n_init=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply the kmeans instance on the normalized, reduced output vector\n",
    "y_km_sample_out = kmeans_1.fit_predict(mat_sample_reduced_out)\n",
    "#apply the kmeans instanceon the normalized, reduced test vector\n",
    "y_km_test_out = kmeans_1.fit_predict(mat_test_reduced_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((collections.Counter(y_km_test_out)),'\\n')\n",
    "#print((collections.Counter(y_km_test_out)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1=collections.Counter(y_km_sample_out)\n",
    "print(type(t1))\n",
    "plt.bar(tl.keys(),t1.values())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1=collections.Counter(y_km_test_out)\n",
    "plt.bar(tl.keys(),t2.values())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd_author_norm = TruncatedSVD(n_components=2000, n_iter=7, random_state=42)\n",
    "mat_reduced_out_author_norm=svd.fit_transform(mat_out_sample_by_author_norm)\n",
    "#print(svd_author_norm.explained_variance_ratio_.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.shape(mat_reduced_out_author_norm))\n",
    "print(np.shape(mat_out_sample_by_author_norm))\n",
    "print(type(mat_reduced_out_author_norm))\n",
    "#print(svd_author_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_test1=KMeans(n_clusters=50,init=mat_reduced_out_author_norm,n_init=1)\n",
    "\n",
    "kmeans_test1.fit(mat_reduced_out_author_norm)\n",
    "kmeans_predict = kmeans_test1.predict(mat_reduced_out)\n",
    "#labels_out_test=kmeans_predict.labels_\n",
    "print(np.shape(kmeans_predict))\n",
    "print(np.shape(mat_reduced_out))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((collections.Counter(labels_out_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialized kmeans instance\n",
    "kmeans=KMeans(n_clusters=50,init='k-means++')\n",
    "\n",
    "\n",
    "y_kmeans.fit(mat_reduced_out)\n",
    "labels_out_test=kmeans_test.labels_\n",
    "\n",
    "print((collections.Counter(labels_out_test)))\n",
    "#print(labels.count(\"1\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tl = {4: 151, 7: 145, 17: 143, 21: 133, 40: 105, 28: 92, 9: 87, 45: 76, 26: 70, 16: 66, 23: 62, 46: 62, 24: 54, 3: 53, 29: 52, 48: 52, 38: 51, 15: 51, 47: 51, 10: 48, 27: 47, 20: 45, 2: 45, 8: 45, 19: 44, 5: 43, 32: 42, 25: 41, 22: 41, 35: 38, 42: 38, 11: 38, 13: 35, 34: 33, 6: 33, 30: 30, 44: 29, 12: 27, 31: 25, 18: 23, 43: 22, 49: 22, 14: 19, 1: 18, 36: 14, 33: 14, 41: 14, 37: 12, 39: 11, 0: 8}\n",
    "plt.bar(tl.keys(),tl.values())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the output of the sample author clustering after dimensional reduction with svd, Counter({4: 151, 7: 145, 17: 143, 21: 133, 40: 105, 28: 92, 9: 87, 45: 76, 26: 70, 16: 66, 23: 62, 46: 62, 24: 54, 3: 53, 29: 52, 48: 52, 38: 51, 15: 51, 47: 51, 10: 48, 27: 47, 20: 45, 2: 45, 8: 45, 19: 44, 5: 43, 32: 42, 25: 41, 22: 41, 35: 38, 42: 38, 11: 38, 13: 35, 34: 33, 6: 33, 30: 30, 44: 29, 12: 27, 31: 25, 18: 23, 43: 22, 49: 22, 14: 19, 1: 18, 36: 14, 33: 14, 41: 14, 37: 12, 39: 11, 0: 8})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_test2=KMeans(n_clusters=50,init=mat_reduced_out)\n",
    "kmeans_test2.fit(mat_out_test_norm)\n",
    "labels_out_test2=kmeans_test.labels_\n",
    "print((collections.Counter(labels_out_test2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
